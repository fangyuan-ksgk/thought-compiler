{"title": "Anatomy of Neural Language Models", "summary": "The fields of generative AI and transfer learning have experienced remarkable\nadvancements in recent years especially in the domain of Natural Language\nProcessing (NLP). Transformers have been at the heart of these advancements\nwhere the cutting-edge transformer-based Language Models (LMs) have led to new\nstate-of-the-art results in a wide spectrum of applications. While the number\nof research works involving neural LMs is exponentially increasing, their vast\nmajority are high-level and far from self-contained. Consequently, a deep\nunderstanding of the literature in this area is a tough task especially in the\nabsence of a unified mathematical framework explaining the main types of neural\nLMs. We address the aforementioned problem in this tutorial where the objective\nis to explain neural LMs in a detailed, simplified and unambiguous mathematical\nframework accompanied by clear graphical illustrations. Concrete examples on\nwidely used models like BERT and GPT2 are explored. Finally, since transformers\npretrained on language-modeling-like tasks have been widely adopted in computer\nvision and time series applications, we briefly explore some examples of such\nsolutions in order to enable readers to understand how transformers work in the\naforementioned domains and compare this use with the original one in NLP.", "tags": ["generative AI", "transfer learning", "natural language processing", "NLP", "transformers", "language models", "neural networks", "BERT", "GPT2", "computer vision", "time series", "deep learning", "mathematical framework"], "citations": [], "date": "2024-01-08", "pdf_path": ""}