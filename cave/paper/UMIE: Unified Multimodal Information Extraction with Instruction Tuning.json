{"title": "UMIE: Unified Multimodal Information Extraction with Instruction Tuning", "summary": "Multimodal information extraction (MIE) gains significant attention as the\npopularity of multimedia content increases. However, current MIE methods often\nresort to using task-specific model structures, which results in limited\ngeneralizability across tasks and underutilizes shared knowledge across MIE\ntasks. To address these issues, we propose UMIE, a unified multimodal\ninformation extractor to unify three MIE tasks as a generation problem using\ninstruction tuning, being able to effectively extract both textual and visual\nmentions. Extensive experiments show that our single UMIE outperforms various\nstate-of-the-art (SoTA) methods across six MIE datasets on three tasks.\nFurthermore, in-depth analysis demonstrates UMIE's strong generalization in the\nzero-shot setting, robustness to instruction variants, and interpretability.\nOur research serves as an initial step towards a unified MIE model and\ninitiates the exploration into both instruction tuning and large language\nmodels within the MIE domain. Our code, data, and model are available at\nhttps://github.com/ZUCC-AI/UMIE", "tags": ["Multimodal information extraction", "Multimedia content", "Instruction tuning", "Generation problem", "Textual mentions", "Visual mentions", "MIE datasets", "State-of-the-art", "Zero-shot learning", "Robustness", "Interpretability", "Unified MIE model", "Large language models", "Machine learning"], "citations": [], "date": "2024-01-05", "pdf_path": ""}