{"title": "Quantifying stability of non-power-seeking in artificial agents", "summary": "We investigate the question: if an AI agent is known to be safe in one\nsetting, is it also safe in a new setting similar to the first? This is a core\nquestion of AI alignment--we train and test models in a certain environment,\nbut deploy them in another, and we need to guarantee that models that seem safe\nin testing remain so in deployment. Our notion of safety is based on\npower-seeking--an agent which seeks power is not safe. In particular, we focus\non a crucial type of power-seeking: resisting shutdown. We model agents as\npolicies for Markov decision processes, and show (in two cases of interest)\nthat not resisting shutdown is \"stable\": if an MDP has certain policies which\ndon't avoid shutdown, the corresponding policies for a similar MDP also don't\navoid shutdown. We also show that there are natural cases where safety is _not_\nstable--arbitrarily small perturbations may result in policies which never shut\ndown. In our first case of interest--near-optimal policies--we use a\nbisimulation metric on MDPs to prove that small perturbations won't make the\nagent take longer to shut down. Our second case of interest is policies for\nMDPs satisfying certain constraints which hold for various models (including\nlanguage models). Here, we demonstrate a quantitative bound on how fast the\nprobability of not shutting down can increase: by defining a metric on MDPs;\nproving that the probability of not shutting down, as a function on MDPs, is\nlower semicontinuous; and bounding how quickly this function decreases.", "tags": ["AI alignment", "AI safety", "power-seeking", "Markov decision processes", "policies", "Machines Learing", "MDPs", "bisimulation", "stability", "lower semicontinuity", "deployment", "testing", "power", "resisting shutdown", "AI"], "citations": [], "date": "2024-01-07", "pdf_path": ""}