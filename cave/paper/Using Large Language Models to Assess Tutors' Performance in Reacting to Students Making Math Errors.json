{"title": "Using Large Language Models to Assess Tutors' Performance in Reacting to Students Making Math Errors", "summary": "Research suggests that tutors should adopt a strategic approach when\naddressing math errors made by low-efficacy students. Rather than drawing\ndirect attention to the error, tutors should guide the students to identify and\ncorrect their mistakes on their own. While tutor lessons have introduced this\npedagogical skill, human evaluation of tutors applying this strategy is arduous\nand time-consuming. Large language models (LLMs) show promise in providing\nreal-time assessment to tutors during their actual tutoring sessions, yet\nlittle is known regarding their accuracy in this context. In this study, we\ninvestigate the capacity of generative AI to evaluate real-life tutors'\nperformance in responding to students making math errors. By analyzing 50\nreal-life tutoring dialogues, we find both GPT-3.5-Turbo and GPT-4 demonstrate\nproficiency in assessing the criteria related to reacting to students making\nerrors. However, both models exhibit limitations in recognizing instances where\nthe student made an error. Notably, GPT-4 tends to overidentify instances of\nstudents making errors, often attributing student uncertainty or inferring\npotential errors where human evaluators did not. Future work will focus on\nenhancing generalizability by assessing a larger dataset of dialogues and\nevaluating learning transfer. Specifically, we will analyze the performance of\ntutors in real-life scenarios when responding to students' math errors before\nand after lesson completion on this crucial tutoring skill.", "tags": [], "citations": [], "date": "2024-01-06", "pdf_path": ""}