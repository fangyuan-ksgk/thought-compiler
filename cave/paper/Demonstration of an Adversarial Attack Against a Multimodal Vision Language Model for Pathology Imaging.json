{"title": "Demonstration of an Adversarial Attack Against a Multimodal Vision Language Model for Pathology Imaging", "summary": "In the context of medical artificial intelligence, this study explores the\nvulnerabilities of the Pathology Language-Image Pretraining (PLIP) model, a\nVision Language Foundation model, under targeted attacks. Leveraging the Kather\nColon dataset with 7,180 H&E images across nine tissue types, our investigation\nemploys Projected Gradient Descent (PGD) adversarial perturbation attacks to\ninduce misclassifications intentionally. The outcomes reveal a 100% success\nrate in manipulating PLIP's predictions, underscoring its susceptibility to\nadversarial perturbations. The qualitative analysis of adversarial examples\ndelves into the interpretability challenges, shedding light on nuanced changes\nin predictions induced by adversarial manipulations. These findings contribute\ncrucial insights into the interpretability, domain adaptation, and\ntrustworthiness of Vision Language Models in medical imaging. The study\nemphasizes the pressing need for robust defenses to ensure the reliability of\nAI models. The source codes for this experiment can be found at\nhttps://github.com/jaiprakash1824/VLM_Adv_Attack.", "tags": ["medical artificial intelligence", "pathology language-image pretraining", "vision language foundation model", "adversarial attacks", "projected gradient descent", "pgd adversarial perturbation attacks", "kather colon dataset", "image classification", "medical imaging", "machine learning", "computer vision", "artificial intelligence", "domain adaptation", "trustworthiness", "robust defenses", "image analysis", "interpretability", "deep learning"], "citations": [], "date": "2024-01-04", "pdf_path": ""}