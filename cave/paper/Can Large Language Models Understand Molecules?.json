{"title": "Can Large Language Models Understand Molecules?", "summary": "Purpose: Large Language Models (LLMs) like GPT (Generative Pre-trained\nTransformer) from OpenAI and LLaMA (Large Language Model Meta AI) from Meta AI\nare increasingly recognized for their potential in the field of\ncheminformatics, particularly in understanding Simplified Molecular Input Line\nEntry System (SMILES), a standard method for representing chemical structures.\nThese LLMs also have the ability to decode SMILES strings into vector\nrepresentations.\n  Method: We investigate the performance of GPT and LLaMA compared to\npre-trained models on SMILES in embedding SMILES strings on downstream tasks,\nfocusing on two key applications: molecular property prediction and drug-drug\ninteraction prediction.\n  Results: We find that SMILES embeddings generated using LLaMA outperform\nthose from GPT in both molecular property and DDI prediction tasks. Notably,\nLLaMA-based SMILES embeddings show results comparable to pre-trained models on\nSMILES in molecular prediction tasks and outperform the pre-trained models for\nthe DDI prediction tasks.\n  Conclusion: The performance of LLMs in generating SMILES embeddings shows\ngreat potential for further investigation of these models for molecular\nembedding. We hope our study bridges the gap between LLMs and molecular\nembedding, motivating additional research into the potential of LLMs in the\nmolecular representation field. GitHub:\nhttps://github.com/sshaghayeghs/LLaMA-VS-GPT", "tags": ["large language models", "GPT", "LLaMA", "cheminformatics", "Simplified Molecular Input Line Entry System", "SMILES", "molecular property prediction", "drug-drug interaction prediction", "embedding", "downstream tasks", "Transformer", "pre-trained models", "embedding SMILES strings", "molecular representation field"], "citations": [], "date": "2024-01-05", "pdf_path": ""}