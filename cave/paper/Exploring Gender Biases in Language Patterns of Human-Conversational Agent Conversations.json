{"title": "Exploring Gender Biases in Language Patterns of Human-Conversational Agent Conversations", "summary": "With the rise of human-machine communication, machines are increasingly\ndesigned with humanlike characteristics, such as gender, which can\ninadvertently trigger cognitive biases. Many conversational agents (CAs), such\nas voice assistants and chatbots, default to female personas, leading to\nconcerns about perpetuating gender stereotypes and inequality. Critiques have\nemerged regarding the potential objectification of females and reinforcement of\ngender stereotypes by these technologies. This research, situated in\nconversational AI design, aims to delve deeper into the impacts of gender\nbiases in human-CA interactions. From a behavioral and communication research\nstandpoint, this program focuses not only on perceptions but also the\nlinguistic styles of users when interacting with CAs, as previous research has\nrarely explored. It aims to understand how pre-existing gender biases might be\ntriggered by CAs' gender designs. It further investigates how CAs' gender\ndesigns may reinforce gender biases and extend them to human-human\ncommunication. The findings aim to inform ethical design of conversational\nagents, addressing whether gender assignment in CAs is appropriate and how to\npromote gender equality in design.", "tags": ["conversational AI", "gender bias", "human-computer interaction", "gender stereotypes", "inequality", "conversational agents", "voice assistants", "chatbots", "human-machine communication", "computational linguistics", "design", "ethics", "gender inequality", "conversation design", "biases", "machine-human interaction"], "citations": [], "date": "2024-01-05", "pdf_path": ""}