{"title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline", "summary": "The commercialization of text-to-image diffusion models (DMs) brings forth\npotential copyright concerns. Despite numerous attempts to protect DMs from\ncopyright issues, the vulnerabilities of these solutions are underexplored. In\nthis study, we formalized the Copyright Infringement Attack on generative AI\nmodels and proposed a backdoor attack method, SilentBadDiffusion, to induce\ncopyright infringement without requiring access to or control over training\nprocesses. Our method strategically embeds connections between pieces of\ncopyrighted information and text references in poisoning data while carefully\ndispersing that information, making the poisoning data inconspicuous when\nintegrated into a clean dataset. Our experiments show the stealth and efficacy\nof the poisoning data. When given specific text prompts, DMs trained with a\npoisoning ratio of 0.20% can produce copyrighted images. Additionally, the\nresults reveal that the more sophisticated the DMs are, the easier the success\nof the attack becomes. These findings underline potential pitfalls in the\nprevailing copyright protection strategies and underscore the necessity for\nincreased scrutiny to prevent the misuse of DMs.", "tags": ["copyright", "text-to-image diffusion models", "generative AI", "backdoor attack", "data poisoning", "machine learning", "deep learning", "artificial intelligence", "copyright infringement", "computer security", "adversarial attack", "normalized text", "natural language processing"], "citations": [], "date": "2024-01-07", "pdf_path": ""}