{"title": "Correctness Comparison of ChatGPT-4, Gemini, Claude-3, and Copilot for Spatial Tasks", "summary": "Generative AI including large language models (LLMs) has recently gained\nsignificant interest in the geo-science community through its versatile\ntask-solving capabilities including programming, arithmetic reasoning,\ngeneration of sample data, time-series forecasting, toponym recognition, or\nimage classification. Most existing performance assessments of LLMs for spatial\ntasks have primarily focused on ChatGPT, whereas other chatbots received less\nattention. To narrow this research gap, this study conducts a zero-shot\ncorrectness evaluation for a set of 76 spatial tasks across seven task\ncategories assigned to four prominent chatbots, i.e., ChatGPT-4, Gemini,\nClaude-3, and Copilot. The chatbots generally performed well on tasks related\nto spatial literacy, GIS theory, and interpretation of programming code and\nfunctions, but revealed weaknesses in mapping, code writing, and spatial\nreasoning. Furthermore, there was a significant difference in correctness of\nresults between the four chatbots. Responses from repeated tasks assigned to\neach chatbot showed a high level of consistency in responses with matching\nrates of over 80% for most task categories in the four chatbots.", "tags": ["generative AI", "large language models (LLMs)", "geo-science", "programming", "arithmetic reasoning", "data generation", "time-series forecasting", "toponym recognition", "image classification", "ChatGPT", "Gemini", "Claude-3", "Copilot", "zero-shot correctness evaluation", "spatial tasks", "spatial literacy", "GIS theory", "spatial reasoning", "code writing", "mapping"], "citations": [], "date": "2024-01-04", "pdf_path": ""}