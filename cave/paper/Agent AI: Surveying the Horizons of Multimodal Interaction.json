{"title": "Agent AI: Surveying the Horizons of Multimodal Interaction", "summary": "Multi-modal AI systems will likely become a ubiquitous presence in our\neveryday lives. A promising approach to making these systems more interactive\nis to embody them as agents within physical and virtual environments. At\npresent, systems leverage existing foundation models as the basic building\nblocks for the creation of embodied agents. Embedding agents within such\nenvironments facilitates the ability of models to process and interpret visual\nand contextual data, which is critical for the creation of more sophisticated\nand context-aware AI systems. For example, a system that can perceive user\nactions, human behavior, environmental objects, audio expressions, and the\ncollective sentiment of a scene can be used to inform and direct agent\nresponses within the given environment. To accelerate research on agent-based\nmultimodal intelligence, we define \"Agent AI\" as a class of interactive systems\nthat can perceive visual stimuli, language inputs, and other\nenvironmentally-grounded data, and can produce meaningful embodied actions. In\nparticular, we explore systems that aim to improve agents based on\nnext-embodied action prediction by incorporating external knowledge,\nmulti-sensory inputs, and human feedback. We argue that by developing agentic\nAI systems in grounded environments, one can also mitigate the hallucinations\nof large foundation models and their tendency to generate environmentally\nincorrect outputs. The emerging field of Agent AI subsumes the broader embodied\nand agentic aspects of multimodal interactions. Beyond agents acting and\ninteracting in the physical world, we envision a future where people can easily\ncreate any virtual reality or simulated scene and interact with agents embodied\nwithin the virtual environment.", "tags": ["multi-modal AI", "agent-based intelligence", "embodied AI", "multimodal interactions", "agentic AI", "agent AI", "foundation models", "hallucinations in AI", "semi-supervised learning", "human feedback", "next-embodied action prediction", "external knowledge", "multi-sensory inputs", "grounded environments", "virtual reality", "simulated scene", "physical environment", "artificial intelligence", "machine learning", "computer vision", "contextual data", "sentiment analysis", "natural language processing"], "citations": [], "date": "2024-01-07", "pdf_path": ""}