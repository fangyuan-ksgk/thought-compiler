{"title": "PokerGPT: An End-to-End Lightweight Solver for Multi-Player Texas Hold'em via Large Language Model", "summary": "Poker, also known as Texas Hold'em, has always been a typical research target\nwithin imperfect information games (IIGs). IIGs have long served as a measure\nof artificial intelligence (AI) development. Representative prior works, such\nas DeepStack and Libratus heavily rely on counterfactual regret minimization\n(CFR) to tackle heads-up no-limit Poker. However, it is challenging for\nsubsequent researchers to learn CFR from previous models and apply it to other\nreal-world applications due to the expensive computational cost of CFR\niterations. Additionally, CFR is difficult to apply to multi-player games due\nto the exponential growth of the game tree size. In this work, we introduce\nPokerGPT, an end-to-end solver for playing Texas Hold'em with arbitrary number\nof players and gaining high win rates, established on a lightweight large\nlanguage model (LLM). PokerGPT only requires simple textual information of\nPoker games for generating decision-making advice, thus guaranteeing the\nconvenient interaction between AI and humans. We mainly transform a set of\ntextual records acquired from real games into prompts, and use them to\nfine-tune a lightweight pre-trained LLM using reinforcement learning human\nfeedback technique. To improve fine-tuning performance, we conduct prompt\nengineering on raw data, including filtering useful information, selecting\nbehaviors of players with high win rates, and further processing them into\ntextual instruction using multiple prompt engineering techniques. Through the\nexperiments, we demonstrate that PokerGPT outperforms previous approaches in\nterms of win rate, model size, training time, and response speed, indicating\nthe great potential of LLMs in solving IIGs.", "tags": ["Artificial intelligence", "Poker", "Texas Hold'em", "Imperfect information games", "Counterfactual regret minimization", "Deep learning", "Large language model", "Reinforcement learning", "Human-computer interaction", "Textual data", "Game theory", "Machine learning", "Game playing", "Natural language processing", "Computational cost", "Game tree size", "Multi-player games", "Language models", "Fine-tuning", "Prompt engineering"], "citations": [], "date": "2024-01-04", "pdf_path": ""}